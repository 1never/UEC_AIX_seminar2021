{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIX_Seminar_2021_Day2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_YmuXwzyaom"
      },
      "source": [
        "[Open with Colab](https://colab.research.google.com/github/1never/UEC_AIX_seminar2021/blob/master/UEC_AIX_seminar2021.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0LEs17_wA4k"
      },
      "source": [
        "**メニュー「ランタイム→ランタイムのタイプを変更」**でハードウェアアクセラレータを**GPU**に変更して保存してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZWt9BJkZair"
      },
      "source": [
        "# GoogleDriveをマウントする\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/My Drive'\n",
        "\n",
        "#学習ファイルのダウンロード\n",
        "!git clone https://github.com/1never/UEC_AIX_seminar2021.git\n",
        "\n",
        "#作業用フォルダの作成\n",
        "!mkdir -p '/content/drive/My Drive/UEC_AIX_seminar2021/'\n",
        "\n",
        "#青空文庫データ保存用フォルダの作成\n",
        "!mkdir -p '/content/drive/My Drive/UEC_AIX_seminar2021/aozora_data'\n",
        "\n",
        "# 学習済みモデル保存用フォルダの作成\n",
        "!mkdir -p '/content/drive/My Drive/UEC_AIX_seminar2021/bert_data'\n",
        "\n",
        "#作業用フォルダに移動する\n",
        "%cd '/content/drive/My Drive/UEC_AIX_seminar2021/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P8dPYhArF_W"
      },
      "source": [
        "**マイドライブ＞UEC_AIX_seminar2021>aozora_data** 内に青空文庫からダウンロードしたzipファイルを入れてください．\n",
        "[青空文庫](https://www.aozora.gr.jp/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beTSr6cSZ5ha"
      },
      "source": [
        "# Huggingface Datasetsのインストール\n",
        "!pip install datasets==1.2.1\n",
        "\n",
        "# Sentencepieceのインストール\n",
        "!pip install sentencepiece==0.1.91\n",
        "\n",
        "# transformersのインストール\n",
        "!pip install transformers==4.4.2 tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBdrbH740it_"
      },
      "source": [
        "# 作業フォルダに戻る\n",
        "%cd '/content/drive/My Drive/UEC_AIX_seminar2021/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZn7cRlbZ-Eu"
      },
      "source": [
        "#aozora_dataファルダに移動\n",
        "%cd '/content/drive/My Drive/UEC_AIX_seminar2021/aozora_data'\n",
        "#フォルダ内の既存のtxtファイルをすべて削除\n",
        "!rm *.txt\n",
        "#フォルダ内のzipファイルを展開する\n",
        "!unzip '*.zip'\n",
        "#作業フォルダに移動\n",
        "%cd '/content/drive/My Drive/UEC_AIX_seminar2021/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6_YZ4eWaKcP"
      },
      "source": [
        "import re\n",
        "import glob\n",
        "\n",
        "#ファインチューニング用のデータを作成する\n",
        "train_text_list = []\n",
        "files = glob.glob('./aozora_data/*.txt')\n",
        "for file in files:\n",
        "  with open(file, encoding=\"shift-jis\") as f:\n",
        "    #最終的にtextは句点区切りの文を要素としてもつリストになる\n",
        "    text = f.read()\n",
        "    text = re.split('-{55}',text)\n",
        "    text = re.split('底本：',text[2])\n",
        "    text = re.sub('《.*》','',text[0])\n",
        "    text = re.sub('［＃.*］','', text)\n",
        "    text = re.split(\"(?<=。)\",text)\n",
        "  #テキストを一文ずつ分割したデータを作成\n",
        "  for sentence in text[0:-1]:\n",
        "    if len(sentence):\n",
        "      train_text_list.append(sentence.strip().replace('\\n',''))\n",
        "\n",
        "#データをtrain.txtとして保存\n",
        "with open(\"train.txt\", mode='w') as f:\n",
        "  f.write('\\n'.join(train_text_list))\n",
        "print(\"Write\", len(train_text_list), \"lines.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxjod_AYaUH6"
      },
      "source": [
        "%%time\n",
        "!rm -r ./output\n",
        "# ファインチューニングの実行\n",
        "!python ./run_clm.py \\\n",
        "    --model_name_or_path=rinna/japanese-gpt2-medium \\\n",
        "    --train_file=train.txt \\\n",
        "    --validation_file=train.txt \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --num_train_epochs=3 \\\n",
        "    --save_steps=5000 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --per_device_train_batch_size=1 \\\n",
        "    --per_device_eval_batch_size=1 \\\n",
        "    --output_dir=output/ \\\n",
        "    --use_fast_tokenizer=False \\\n",
        "    --block_size 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpscAmQ3sI3b"
      },
      "source": [
        "**GPT2のみを用いた生成**\n",
        "\n",
        "1回目の実行は失敗する場合があるので，その場合は2回実行してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdygRzqTPDjB"
      },
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "import re\n",
        "\n",
        "# ファインチューニングしたモデルを用いる\n",
        "USE_FINETUNED_GPT2 = True\n",
        "# ファインチューニングしたモデルを用いない\n",
        "# USE_FINETUNED_GPT2 = False\n",
        "\n",
        "#候補文をいくつ表示するか\n",
        "OPTION_NUM = 4\n",
        "\n",
        "# トークナイザーの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "#モデルの準備\n",
        "if USE_FINETUNED_GPT2:\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "else:\n",
        "  model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "\n",
        "u = \"\"\n",
        "next_sentence = input(\"\\n>\")\n",
        "log = []\n",
        "log.append(next_sentence)\n",
        "\n",
        "while(True):  \n",
        "  if \"exit\" == u:\n",
        "    break\n",
        "  if \"back\" == u:\n",
        "    log.pop()\n",
        "    next_sentence = re.split(\"(?<=。)\",log[-1])[0]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if \"log\" == u:\n",
        "    print(\"ログ：\")\n",
        "    print(\"\\n\")\n",
        "    print(\"\\n\".join(log))\n",
        "    print(\"\\n\")\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if not \"。\" in next_sentence:\n",
        "    next_sentence = next_sentence + \"。\" \n",
        "  # 推論\n",
        "  encoded = tokenizer.encode(next_sentence, return_tensors=\"pt\")\n",
        "  output = model.generate(encoded, do_sample=True, max_length=100, num_return_sequences=OPTION_NUM)\n",
        "\n",
        "  sequence_list = []\n",
        "  for sequence in tokenizer.batch_decode(output):\n",
        "    sequence = sequence.replace('</s>', '')\n",
        "    sentence_list = re.split(\"(?<=。)\",sequence)[:-1]\n",
        "    sequence = \"\".join(sentence_list)\n",
        "    sequence_list.append(sequence)\n",
        "  for i,sequence in enumerate(sequence_list):\n",
        "    print(\"[\", i,\"]\",sequence)\n",
        "  \n",
        "  u = input(\"\\n>\")\n",
        "  if u.isdecimal():\n",
        "    choice_sequence = sequence_list[int(u)]\n",
        "    log.append(choice_sequence[len(next_sentence):])\n",
        "    next_sentence = re.split(\"(?<=。)\", choice_sequence)[-2]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "\n",
        "  # 1. \">\"の右の入力欄に最初の一文を入力します\n",
        "  # 2. 入力文に続く[0]～[3]までの候補文が表示されます\n",
        "  # 3. 表示された候補の左の数字を\">\"の右の入力欄に入力することでその候補の最後の文が次の入力文になります\n",
        "  # 4. \"log\"と入力するとこれまでの文章が続けて表示されます\n",
        "  # 5. \"back\"と入力すると入力が一つ前まで戻ります\n",
        "  # 6. \"exit\"と入力すると終了します"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReW2XN6orDxa"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "**ここからBERTの学習とそれを活用した推論**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvrB6gXYcfHd"
      },
      "source": [
        "#ライブラリのインストール\n",
        "!apt install git make curl xz-utils file\n",
        "!apt install mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "!pip install mecab-python3==0.996.5\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikU7fNEDc-4g"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "import random\n",
        "\n",
        "#aozora_dataフォルダ内のtxtファイルをつかってpairデータを作成する\n",
        "pair_list = []\n",
        "files = glob.glob('./aozora_data/*.txt')\n",
        "for file in files:\n",
        "  with open(file, encoding=\"shift-jis\") as f:\n",
        "    #最終的にtextは句点区切りの文を要素としてもつリストになる\n",
        "    text = f.read()\n",
        "    text = re.split('-{55}',text)\n",
        "    text = re.split('底本：',text[2])\n",
        "    text = re.sub('《.*》','',text[0])\n",
        "    text = re.sub('［＃.*］','', text)\n",
        "    text = re.split(\"(?<=。)\",text)\n",
        "    #1～3文と1～3文が対応するペアデータを作成\n",
        "    for i in range(len(text)-6):\n",
        "      m = random.randint(1,3)\n",
        "      n = random.randint(1,3)\n",
        "      pair_list.append(\"\".join(text[i:i+m])+\"\\t\"+\"\".join(text[i+m:i+m+n]))\n",
        "\n",
        "#ペアデータをpair.txtとして保存\n",
        "with open(\"pair.txt\", mode='w') as f:\n",
        "  f.write('\\n'.join(pair_list))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p1H7AxOdCDO"
      },
      "source": [
        "write_lines = []\n",
        "uttrs = []\n",
        "\n",
        "filename = 'pair.txt'\n",
        "\n",
        "with open(filename) as f:\n",
        "    for l in f:\n",
        "        l = l.strip()\n",
        "        if \"\\t\" in l:\n",
        "            # 実際の応答ペアを正解とし，ラベルは1とする．\n",
        "            write_lines.append('1,' + l.replace(\"\\t\", \",\") + \"\\n\")\n",
        "            print('1,' + l.replace(\"\\t\", \",\") + ',' + \"\\n\")\n",
        "            # 不正解ペアの作成のため，発話を保存\n",
        "            uttrs.append(l.split(\"\\t\")[0])\n",
        "            uttrs.append(l.split(\"\\t\")[1])\n",
        "  \n",
        "# 正解ペアと同じ数だけ不正解ペアを作成\n",
        "for i in range(len(write_lines)):\n",
        "    # ランダムな応答ペアを不正解とし，ラベルは0とする．\n",
        "    write_lines.append('0,' + random.choice(uttrs) + \",\" + random.choice(uttrs) + \"\\n\")\n",
        "  \n",
        " # 正解ペアと不正解ペアが入ったリストをシャッフルする\n",
        "random.shuffle(write_lines)\n",
        "  \n",
        "index = 0\n",
        "with open(\"bert_data/dev.csv\", \"w\") as var_f:\n",
        "    var_f.write(\"label,sentence1,sentence2\\n\")\n",
        "    # 開発データとしてdev.tsvに200行を書き込む．\n",
        "    for l in write_lines[:200]:\n",
        "        var_f.write(l)\n",
        "        index += 1\n",
        "index = 0\n",
        "with open(\"bert_data/train.csv\", \"w\") as var_f:\n",
        "    var_f.write(\"label,sentence1,sentence2\\n\")\n",
        "    # 学習データとしてtrain.tsvにのこりを書き込む．\n",
        "    for l in write_lines[200:]:\n",
        "        var_f.write(l)\n",
        "        index += 1\n",
        "print(\"Write\", len(write_lines[200:]), \"lines to train.tsv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBubSjUadMCI"
      },
      "source": [
        "# max_stepsの値を大きな値に設定することで，より多くのデータで学習できるが，より多くの時間が必要となる．\n",
        "# 時間がかかるので100に設定しているが，実際は全然足らない．10000以上には設定したい．\n",
        "!python ./run_glue.py --train_file ./bert_data/train.csv --validation_file ./bert_data/dev.csv  --overwrite_output_dir --overwrite_cache \\\n",
        "--model_name_or_path cl-tohoku/bert-base-japanese-whole-word-masking  --save_steps 100 --max_steps 100 \\\n",
        "--output_dir bert_output/ --do_train --do_eval --per_gpu_train_batch_size 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrI5QftFgccn"
      },
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class BertEvaluator:\n",
        "    def __init__(self):\n",
        "        # 事前学習済みのトークナイザとモデルをロード\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', do_lower_case=False)\n",
        "        self.model = BertForSequenceClassification.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', num_labels=2)\n",
        "        \n",
        "        # Google Colabでファインチューニングしたモデルをロード\n",
        "        self.model.load_state_dict(torch.load(\"bert_output/pytorch_model.bin\", map_location=\"cpu\"))\n",
        "        self.model.to(device)\n",
        "\n",
        "    def evaluate(self, user_input, candidate):\n",
        "        with torch.no_grad():\n",
        "            # 発話のペアを特徴ベクトルに変換\n",
        "            tokenized = self.tokenizer([(user_input, candidate)], return_tensors=\"pt\", padding=True)\n",
        "            input_ids = tokenized[\"input_ids\"].to(device)\n",
        "            token_type_ids = tokenized[\"token_type_ids\"].to(device)\n",
        "\n",
        "            # ファインチューニング済みのBERTを用いて特徴ベクトルから2文のスコアを計算\n",
        "            result = self.model.forward(input_ids, token_type_ids=token_type_ids)\n",
        "            # softmax関数によりスコアを正規化\n",
        "            result = F.softmax(result[0], dim=1).cpu().numpy().tolist()\n",
        "\n",
        "            # 結果を返す．\n",
        "            return result[0][1]\n",
        "\n",
        "#BERTを使った評価用クラスの準備\n",
        "be = BertEvaluator()\n",
        "\n",
        "# テスト\n",
        "sentence1 = \"おはよう。\"\n",
        "sentence2 = \"いい朝ですね。\"\n",
        "sentence3 = \"悲しいです。\"\n",
        "\n",
        "score1 = be.evaluate(sentence1, sentence2)\n",
        "print(f\"「{sentence1}」と「{sentence2}」の一貫性:\", score1)\n",
        "score2 = be.evaluate(sentence2, sentence3)\n",
        "print(f\"「{sentence2}」と「{sentence3}」の一貫性:\", score2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxXvQbK9gisG"
      },
      "source": [
        "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
        "import re\n",
        "\n",
        "#文章をいくつ推論するか\n",
        "RETURN_NUM = 8\n",
        "#候補文をいくつ表示するか(RETURN_NUMより小さい値にしてください)\n",
        "OPTION_NUM = 4\n",
        "\n",
        "# トークナイザーとモデルの準備\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"output/\")\n",
        "\n",
        "\n",
        "u = \"\"\n",
        "next_sentence = input(\"\\n>\")\n",
        "log = []\n",
        "log.append(next_sentence)\n",
        "\n",
        "while(True):  \n",
        "  if \"exit\" == u:\n",
        "    break\n",
        "  if \"back\" == u:\n",
        "    log.pop()\n",
        "    next_sentence = re.split(\"(?<=。)\",log[-1])[0]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if \"log\" == u:\n",
        "    print(\"ログ：\")\n",
        "    print(\"\\n\")\n",
        "    print(\"\\n\".join(log))\n",
        "    print(\"\\n\")\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "  if not \"。\" in next_sentence:\n",
        "    next_sentence = next_sentence + \"。\" \n",
        "  # 推論\n",
        "  encoded = tokenizer.encode(next_sentence, return_tensors=\"pt\")\n",
        "  output = model.generate(encoded, do_sample=True, max_length=100, num_return_sequences=RETURN_NUM)\n",
        "\n",
        "  sequence_dict = {}\n",
        "  for sequence in tokenizer.batch_decode(output):\n",
        "    sequence = sequence.replace('</s>', '')\n",
        "    sentence_list = re.split(\"(?<=。)\",sequence)[:-1]\n",
        "    sequence = \"\".join(sentence_list)\n",
        "    score = be.evaluate(\"\".join(log[-1]), sequence[len(next_sentence):])\n",
        "    sequence_dict[sequence] = score\n",
        "  sequence_dict = sorted(sequence_dict.items(), key=lambda x:x[1], reverse=True)\n",
        "  for i in range(OPTION_NUM):\n",
        "    print(\"[\", i,\"]\", sequence_dict[i][0])\n",
        "  \n",
        "  u = input(\"\\n>\")\n",
        "  if u.isdecimal():\n",
        "    choice_sequence = sequence_dict[int(u)][0]\n",
        "    log.append(choice_sequence[len(next_sentence):])\n",
        "    next_sentence = re.split(\"(?<=。)\", choice_sequence)[-2]\n",
        "    print(\"入力文：\")\n",
        "    print(next_sentence)\n",
        "\n",
        "  # 1. \">\"の右の入力欄に最初の一文を入力します\n",
        "  # 2. 入力文に続く[0]～[3]までの候補文が表示されます\n",
        "  # 3. 表示された候補の左の数字を\">\"の右の入力欄に入力することでその候補の最後の文が次の入力文になります\n",
        "  # 4. \"log\"と入力するとこれまでの文章が続けて表示されます\n",
        "  # 5. \"back\"と入力すると入力が一つ前まで戻ります\n",
        "  # 6. \"exit\"と入力すると終了します"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}